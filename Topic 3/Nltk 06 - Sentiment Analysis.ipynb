{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nltk Sentiment Analysis\n",
    "\n",
    "https://www.nltk.org/api/nltk.sentiment.html\n",
    "\n",
    "\n",
    "Sentiment analysis is used to determine positive or negative sentiment in text.\n",
    "\n",
    "In this notebook I will decompose the sample code in the NTLK documentation so we can understand what it is doing:\n",
    "\n",
    "https://www.nltk.org/howto/sentiment.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sentiment analyser class\n",
    "from nltk.sentiment import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always get help on any Python object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SentimentAnalyzer in module nltk.sentiment.sentiment_analyzer:\n",
      "\n",
      "class SentimentAnalyzer(builtins.object)\n",
      " |  SentimentAnalyzer(classifier=None)\n",
      " |  \n",
      " |  A Sentiment Analysis tool based on machine learning approaches.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, classifier=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_feat_extractor(self, function, **kwargs)\n",
      " |      Add a new function to extract features from a document. This function will\n",
      " |      be used in extract_features().\n",
      " |      Important: in this step our kwargs are only representing additional parameters,\n",
      " |      and NOT the document we have to parse. The document will always be the first\n",
      " |      parameter in the parameter list, and it will be added in the extract_features()\n",
      " |      function.\n",
      " |      \n",
      " |      :param function: the extractor function to add to the list of feature extractors.\n",
      " |      :param kwargs: additional parameters required by the `function` function.\n",
      " |  \n",
      " |  all_words(self, documents, labeled=None)\n",
      " |      Return all words/tokens from the documents (with duplicates).\n",
      " |      \n",
      " |      :param documents: a list of (words, label) tuples.\n",
      " |      :param labeled: if `True`, assume that each document is represented by a\n",
      " |          (words, label) tuple: (list(str), str). If `False`, each document is\n",
      " |          considered as being a simple list of strings: list(str).\n",
      " |      :rtype: list(str)\n",
      " |      :return: A list of all words/tokens in `documents`.\n",
      " |  \n",
      " |  apply_features(self, documents, labeled=None)\n",
      " |      Apply all feature extractor functions to the documents. This is a wrapper\n",
      " |      around `nltk.classify.util.apply_features`.\n",
      " |      \n",
      " |      If `labeled=False`, return featuresets as:\n",
      " |          [feature_func(doc) for doc in documents]\n",
      " |      If `labeled=True`, return featuresets as:\n",
      " |          [(feature_func(tok), label) for (tok, label) in toks]\n",
      " |      \n",
      " |      :param documents: a list of documents. `If labeled=True`, the method expects\n",
      " |          a list of (words, label) tuples.\n",
      " |      :rtype: LazyMap\n",
      " |  \n",
      " |  bigram_collocation_feats(self, documents, top_n=None, min_freq=3, assoc_measure=<bound method NgramAssocMeasures.pmi of <class 'nltk.metrics.association.BigramAssocMeasures'>>)\n",
      " |      Return `top_n` bigram features (using `assoc_measure`).\n",
      " |      Note that this method is based on bigram collocations measures, and not\n",
      " |      on simple bigram frequency.\n",
      " |      \n",
      " |      :param documents: a list (or iterable) of tokens.\n",
      " |      :param top_n: number of best words/tokens to use, sorted by association\n",
      " |          measure.\n",
      " |      :param assoc_measure: bigram association measure to use as score function.\n",
      " |      :param min_freq: the minimum number of occurrencies of bigrams to take\n",
      " |          into consideration.\n",
      " |      \n",
      " |      :return: `top_n` ngrams scored by the given association measure.\n",
      " |  \n",
      " |  classify(self, instance)\n",
      " |      Classify a single instance applying the features that have already been\n",
      " |      stored in the SentimentAnalyzer.\n",
      " |      \n",
      " |      :param instance: a list (or iterable) of tokens.\n",
      " |      :return: the classification result given by applying the classifier.\n",
      " |  \n",
      " |  evaluate(self, test_set, classifier=None, accuracy=True, f_measure=True, precision=True, recall=True, verbose=False)\n",
      " |      Evaluate and print classifier performance on the test set.\n",
      " |      \n",
      " |      :param test_set: A list of (tokens, label) tuples to use as gold set.\n",
      " |      :param classifier: a classifier instance (previously trained).\n",
      " |      :param accuracy: if `True`, evaluate classifier accuracy.\n",
      " |      :param f_measure: if `True`, evaluate classifier f_measure.\n",
      " |      :param precision: if `True`, evaluate classifier precision.\n",
      " |      :param recall: if `True`, evaluate classifier recall.\n",
      " |      :return: evaluation results.\n",
      " |      :rtype: dict(str): float\n",
      " |  \n",
      " |  extract_features(self, document)\n",
      " |      Apply extractor functions (and their parameters) to the present document.\n",
      " |      We pass `document` as the first parameter of the extractor functions.\n",
      " |      If we want to use the same extractor function multiple times, we have to\n",
      " |      add it to the extractors with `add_feat_extractor` using multiple sets of\n",
      " |      parameters (one for each call of the extractor function).\n",
      " |      \n",
      " |      :param document: the document that will be passed as argument to the\n",
      " |          feature extractor functions.\n",
      " |      :return: A dictionary of populated features extracted from the document.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  save_file(self, content, filename)\n",
      " |      Store `content` in `filename`. Can be used to store a SentimentAnalyzer.\n",
      " |  \n",
      " |  train(self, trainer, training_set, save_classifier=None, **kwargs)\n",
      " |      Train classifier on the training set, optionally saving the output in the\n",
      " |      file specified by `save_classifier`.\n",
      " |      Additional arguments depend on the specific trainer used. For example,\n",
      " |      a MaxentClassifier can use `max_iter` parameter to specify the number\n",
      " |      of iterations, while a NaiveBayesClassifier cannot.\n",
      " |      \n",
      " |      :param trainer: `train` method of a classifier.\n",
      " |          E.g.: NaiveBayesClassifier.train\n",
      " |      :param training_set: the training set to be passed as argument to the\n",
      " |          classifier `train` method.\n",
      " |      :param save_classifier: the filename of the file where the classifier\n",
      " |          will be stored (optional).\n",
      " |      :param kwargs: additional parameters that will be passed as arguments to\n",
      " |          the classifier `train` function.\n",
      " |      :return: A classifier instance trained on the training set.\n",
      " |      :rtype:\n",
      " |  \n",
      " |  unigram_word_feats(self, words, top_n=None, min_freq=0)\n",
      " |      Return most common top_n word features.\n",
      " |      \n",
      " |      :param words: a list of words/tokens.\n",
      " |      :param top_n: number of best words/tokens to use, sorted by frequency.\n",
      " |      :rtype: list(str)\n",
      " |      :return: A list of `top_n` words/tokens (with no duplicates) sorted by\n",
      " |          frequency.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SentimentAnalyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Naive Bayes Classification algorithm\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Import the subjectivity test corpus\n",
    "from nltk.corpus import subjectivity\n",
    "\n",
    "# Import the sentiment analysis libraries\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "\n",
    "# Import the utilities library\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab our data set\n",
    "The NTLK subjectivity corpus contains sample texts pre-classified as subjective or objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\subjectivity.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download subjectivity corpus\n",
    "import nltk\n",
    "nltk.download('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LazyCorpusLoader in module nltk.corpus.util object:\n",
      "\n",
      "subjectivity = class CategorizedSentencesCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.api.CorpusReader)\n",
      " |  subjectivity(root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL), sent_tokenizer=None, encoding='utf8', **kwargs)\n",
      " |  \n",
      " |  A reader for corpora in which each row represents a single instance, mainly\n",
      " |  a sentence. Istances are divided into categories based on their file identifiers\n",
      " |  (see CategorizedCorpusReader).\n",
      " |  Since many corpora allow rows that contain more than one sentence, it is\n",
      " |  possible to specify a sentence tokenizer to retrieve all sentences instead\n",
      " |  than all rows.\n",
      " |  \n",
      " |  Examples using the Subjectivity Dataset:\n",
      " |  \n",
      " |  >>> from nltk.corpus import subjectivity\n",
      " |  >>> subjectivity.sents()[23]\n",
      " |  ['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',\n",
      " |  'happened', 'off', 'screen', '.']\n",
      " |  >>> subjectivity.categories()\n",
      " |  ['obj', 'subj']\n",
      " |  >>> subjectivity.words(categories='subj')\n",
      " |  ['smart', 'and', 'alert', ',', 'thirteen', ...]\n",
      " |  \n",
      " |  Examples using the Sentence Polarity Dataset:\n",
      " |  \n",
      " |  >>> from nltk.corpus import sentence_polarity\n",
      " |  >>> sentence_polarity.sents()\n",
      " |  [['simplistic', ',', 'silly', 'and', 'tedious', '.'], [\"it's\", 'so', 'laddish',\n",
      " |  'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',\n",
      " |  'it', 'funny', '.'], ...]\n",
      " |  >>> sentence_polarity.categories()\n",
      " |  ['neg', 'pos']\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CategorizedSentencesCorpusReader\n",
      " |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root, fileids, word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL), sent_tokenizer=None, encoding='utf8', **kwargs)\n",
      " |      :param root: The root directory for the corpus.\n",
      " |      :param fileids: a list or regexp specifying the fileids in the corpus.\n",
      " |      :param word_tokenizer: a tokenizer for breaking sentences or paragraphs\n",
      " |          into words. Default: `WhitespaceTokenizer`\n",
      " |      :param sent_tokenizer: a tokenizer for breaking paragraphs into sentences.\n",
      " |      :param encoding: the encoding that should be used to read the corpus.\n",
      " |      :param kwargs: additional parameters passed to CategorizedCorpusReader.\n",
      " |  \n",
      " |  sents(self, fileids=None, categories=None)\n",
      " |      Return all sentences in the corpus or in the specified file(s).\n",
      " |      \n",
      " |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      " |          sentences have to be returned.\n",
      " |      :param categories: a list specifying the categories whose sentences have\n",
      " |          to be returned.\n",
      " |      :return: the given file(s) as a list of sentences.\n",
      " |          Each sentence is tokenized using the specified word_tokenizer.\n",
      " |      :rtype: list(list(str))\n",
      " |  \n",
      " |  words(self, fileids=None, categories=None)\n",
      " |      Return all words and punctuation symbols in the corpus or in the specified\n",
      " |      file(s).\n",
      " |      \n",
      " |      :param fileids: a list or regexp specifying the ids of the files whose\n",
      " |          words have to be returned.\n",
      " |      :param categories: a list specifying the categories whose words have to\n",
      " |          be returned.\n",
      " |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      " |      :rtype: list(str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      " |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      " |      it can be accessed by index, iterated over, etc.  However, the\n",
      " |      tokens are only constructed as-needed -- the entire corpus is\n",
      " |      never stored in memory at once.\n",
      " |      \n",
      " |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      " |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      " |      and a block reader.  A \"block reader\" is a function that reads\n",
      " |      zero or more tokens from a stream, and returns them as a list.  A\n",
      " |      very simple example of a block reader is:\n",
      " |      \n",
      " |          >>> def simple_block_reader(stream):\n",
      " |          ...     return stream.readline().split()\n",
      " |      \n",
      " |      This simple block reader reads a single line at a time, and\n",
      " |      returns a single token (consisting of a string) for each\n",
      " |      whitespace-separated substring on the line.\n",
      " |      \n",
      " |      When deciding how to define the block reader for a given\n",
      " |      corpus, careful consideration should be given to the size of\n",
      " |      blocks handled by the block reader.  Smaller block sizes will\n",
      " |      increase the memory requirements of the corpus view's internal\n",
      " |      data structures (by 2 integers per block).  On the other hand,\n",
      " |      larger block sizes may decrease performance for random access to\n",
      " |      the corpus.  (But note that larger block sizes will *not*\n",
      " |      decrease performance for iteration.)\n",
      " |      \n",
      " |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      " |      index to file position, with one entry per block.  When a token\n",
      " |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      " |      it as follows:\n",
      " |      \n",
      " |        1. First, it searches the toknum/filepos mapping for the token\n",
      " |           index closest to (but less than or equal to) *i*.\n",
      " |      \n",
      " |        2. Then, starting at the file position corresponding to that\n",
      " |           index, it reads one block at a time using the block reader\n",
      " |           until it reaches the requested token.\n",
      " |      \n",
      " |      The toknum/filepos mapping is created lazily: it is initially\n",
      " |      empty, but every time a new block is read, the block's\n",
      " |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      " |      map has one entry per block.)\n",
      " |      \n",
      " |      In order to increase efficiency for random access patterns that\n",
      " |      have high degrees of locality, the corpus view may cache one or\n",
      " |      more blocks.\n",
      " |      \n",
      " |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      " |          object for its underlying corpus file.  This file should be\n",
      " |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      " |          but if you wish to close it manually, use the ``close()``\n",
      " |          method.  If you access a ``CorpusView``'s items after it has been\n",
      " |          closed, the file object will be automatically re-opened.\n",
      " |      \n",
      " |      :warning: If the contents of the file are modified during the\n",
      " |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      " |          is undefined.\n",
      " |      \n",
      " |      :warning: If a unicode encoding is specified when constructing a\n",
      " |          ``CorpusView``, then the block reader may only call\n",
      " |          ``stream.seek()`` with offsets that have been returned by\n",
      " |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      " |          relative offsets, or with offsets based on string lengths, may\n",
      " |          lead to incorrect behavior.\n",
      " |      \n",
      " |      :ivar _block_reader: The function used to read\n",
      " |          a single block from the underlying file stream.\n",
      " |      :ivar _toknum: A list containing the token index of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          token index of the first token in block ``i``.  Together\n",
      " |          with ``_filepos``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _filepos: A list containing the file position of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          file position of the first character in block ``i``.  Together\n",
      " |          with ``_toknum``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      " |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      " |          or None, if the number of tokens is not yet known.\n",
      " |      :ivar _eofpos: The character position of the last character in the\n",
      " |          file.  This is calculated when the corpus view is initialized,\n",
      " |          and is used to decide when the end of file has been reached.\n",
      " |      :ivar _cache: A cache of the most recently read block.  It\n",
      " |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      " |         start_toknum is the token index of the first token in the block;\n",
      " |         end_toknum is the token index of the first token not in the\n",
      " |         block; and tokens is a list of the tokens in the block.\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      " |  \n",
      " |  categories(self, fileids=None)\n",
      " |      Return a list of the categories that are defined for this corpus,\n",
      " |      or for the file(s) if it is given.\n",
      " |  \n",
      " |  fileids(self, categories=None)\n",
      " |      Return a list of file identifiers for the files that make up\n",
      " |      this corpus, or that make up the given category(s) if specified.\n",
      " |  \n",
      " |  paras(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  raw(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  citation(self)\n",
      " |      Return the contents of the corpus citation.bib file, if it exists.\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  license(self)\n",
      " |      Return the contents of the corpus LICENSE file, if it exists.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  readme(self)\n",
      " |      Return the contents of the corpus README file, if it exists.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the sentences in the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split into subjective and objective sentences.  Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity.sents(categories='subj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity.sents(categories='obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 100 of each (subjective and objective)\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be scientific about the process, we split the documents into a training set, used to build the model, and a test set, used only for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and test set for subjective docs.  80% go in training, save 20% for testing\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "\n",
    "# Create a training and test set for objective docs.  80% go in training, save 20% for testing\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "\n",
    "# Merge the training sets and the test sets\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs),len(testing_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analyser\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Mark up the negative sections of the doc and return just the words\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see a line like the above, it's worth breaking it apart to see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a_NEG',\n",
       "  'mass-market_NEG',\n",
       "  'entertainment_NEG',\n",
       "  'but_NEG',\n",
       "  'an_NEG',\n",
       "  'uncompromising_NEG',\n",
       "  'attempt_NEG',\n",
       "  'by_NEG',\n",
       "  'one_NEG',\n",
       "  'artist_NEG',\n",
       "  'to_NEG',\n",
       "  'think_NEG',\n",
       "  'about_NEG',\n",
       "  'another_NEG',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK: to see what mark_negation does by isolating one document\n",
    "mark_negation(training_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method all_words in module nltk.sentiment.sentiment_analyzer:\n",
      "\n",
      "all_words(documents, labeled=None) method of nltk.sentiment.sentiment_analyzer.SentimentAnalyzer instance\n",
      "    Return all words/tokens from the documents (with duplicates).\n",
      "    \n",
      "    :param documents: a list of (words, label) tuples.\n",
      "    :param labeled: if `True`, assume that each document is represented by a\n",
      "        (words, label) tuple: (list(str), str). If `False`, each document is\n",
      "        considered as being a simple list of strings: list(str).\n",
      "    :rtype: list(str)\n",
      "    :return: A list of all words/tokens in `documents`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get help on all_words\n",
    "help(sentim_analyzer.all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a_NEG',\n",
       " 'mass-market_NEG',\n",
       " 'entertainment_NEG',\n",
       " 'but_NEG',\n",
       " 'an_NEG',\n",
       " 'uncompromising_NEG',\n",
       " 'attempt_NEG',\n",
       " 'by_NEG',\n",
       " 'one_NEG',\n",
       " 'artist_NEG',\n",
       " 'to_NEG',\n",
       " 'think_NEG',\n",
       " 'about_NEG',\n",
       " 'another_NEG',\n",
       " '.',\n",
       " 's',\n",
       " 'u',\n",
       " 'b',\n",
       " 'j']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK: to see what all_words does by isolating one document\n",
    "sentim_analyzer.all_words(mark_negation(training_docs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart',\n",
       " 'and',\n",
       " 'alert',\n",
       " ',',\n",
       " 'thirteen',\n",
       " 'conversations',\n",
       " 'about',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'small',\n",
       " 'gem',\n",
       " '.',\n",
       " 'color',\n",
       " ',',\n",
       " 'musical',\n",
       " 'bounce',\n",
       " 'and',\n",
       " 'warm',\n",
       " 'seas',\n",
       " 'lapping',\n",
       " 'on',\n",
       " 'island',\n",
       " 'shores',\n",
       " '.',\n",
       " 'and',\n",
       " 'just',\n",
       " 'enough',\n",
       " 'science',\n",
       " 'to',\n",
       " 'send',\n",
       " 'you',\n",
       " 'home',\n",
       " 'thinking',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a_NEG',\n",
       " 'mass-market_NEG',\n",
       " 'entertainment_NEG',\n",
       " 'but_NEG',\n",
       " 'an_NEG',\n",
       " 'uncompromising_NEG',\n",
       " 'attempt_NEG',\n",
       " 'by_NEG',\n",
       " 'one_NEG',\n",
       " 'artist_NEG',\n",
       " 'to_NEG']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK: What is in all_words_neg now?\n",
    "all_words_neg[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3799"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return all words that occur at least 4 times\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'the',\n",
       " ',',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'with',\n",
       " 'it',\n",
       " 'that',\n",
       " 'his',\n",
       " 'on',\n",
       " 'for',\n",
       " 'an',\n",
       " 'who',\n",
       " 'by',\n",
       " 'he',\n",
       " 'from',\n",
       " 'her',\n",
       " '\"',\n",
       " 'film',\n",
       " 'as',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'their',\n",
       " 'but',\n",
       " 'one',\n",
       " 'at',\n",
       " 'about',\n",
       " 'the_NEG',\n",
       " 'a_NEG',\n",
       " 'to_NEG',\n",
       " 'are',\n",
       " \"there's\",\n",
       " '(',\n",
       " 'story',\n",
       " 'when',\n",
       " 'so',\n",
       " 'be',\n",
       " ',_NEG',\n",
       " ')',\n",
       " 'they',\n",
       " 'you',\n",
       " 'not',\n",
       " 'have',\n",
       " 'like',\n",
       " 'will',\n",
       " 'all']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at these words\n",
    "unigram_feats[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the extract_unigram_feats feature extractor when processing the doc\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the feature extractor to the training and test docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature extractor to the docs\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'contains(.)': True, 'contains(the)': False, 'contains(,)': True, 'contains(a)': True, 'contains(and)': True, 'contains(of)': False, 'contains(to)': False, 'contains(is)': True, 'contains(in)': False, 'contains(with)': False, 'contains(it)': False, 'contains(that)': False, 'contains(his)': False, 'contains(on)': False, 'contains(for)': False, 'contains(an)': False, 'contains(who)': False, 'contains(by)': False, 'contains(he)': False, 'contains(from)': False, 'contains(her)': False, 'contains(\")': False, 'contains(film)': False, 'contains(as)': False, 'contains(this)': False, 'contains(movie)': False, 'contains(their)': False, 'contains(but)': False, 'contains(one)': True, 'contains(at)': False, 'contains(about)': True, 'contains(the_NEG)': False, 'contains(a_NEG)': False, 'contains(to_NEG)': False, 'contains(are)': False, \"contains(there's)\": False, 'contains(()': False, 'contains(story)': False, 'contains(when)': False, 'contains(so)': False, 'contains(be)': False, 'contains(,_NEG)': False, 'contains())': False, 'contains(they)': False, 'contains(you)': False, 'contains(not)': False, 'contains(have)': False, 'contains(like)': False, 'contains(will)': False, 'contains(all)': False, 'contains(into)': False, 'contains(out)': False, 'contains(she)': False, 'contains(what)': False, 'contains(life)': False, 'contains(has)': False, 'contains(its)': False, 'contains(only)': False, 'contains(more)': False, 'contains(even)': False, 'contains(--)': False, 'contains(:)': False, 'contains(can)': False, 'contains(;)': False, 'contains(home)': False, 'contains(look)': False, \"contains(it's)\": False, 'contains(if)': False, 'contains(where)': False, 'contains(most)': False, 'contains(him)': False, 'contains(search)': False, 'contains(but_NEG)': False, 'contains(love)': False, 'contains(both)': False, 'contains(make)': False, 'contains(begins)': False, 'contains(some)': False, 'contains(two)': False, 'contains(of_NEG)': False, 'contains(made)': False, 'contains(which)': False, 'contains(them)': False}, 'subj'), ({'contains(.)': True, 'contains(the)': False, 'contains(,)': True, 'contains(a)': False, 'contains(and)': True, 'contains(of)': False, 'contains(to)': True, 'contains(is)': False, 'contains(in)': False, 'contains(with)': False, 'contains(it)': False, 'contains(that)': False, 'contains(his)': False, 'contains(on)': True, 'contains(for)': False, 'contains(an)': False, 'contains(who)': False, 'contains(by)': False, 'contains(he)': False, 'contains(from)': False, 'contains(her)': False, 'contains(\")': False, 'contains(film)': False, 'contains(as)': False, 'contains(this)': False, 'contains(movie)': False, 'contains(their)': False, 'contains(but)': False, 'contains(one)': False, 'contains(at)': False, 'contains(about)': False, 'contains(the_NEG)': False, 'contains(a_NEG)': False, 'contains(to_NEG)': False, 'contains(are)': False, \"contains(there's)\": False, 'contains(()': False, 'contains(story)': False, 'contains(when)': False, 'contains(so)': False, 'contains(be)': False, 'contains(,_NEG)': False, 'contains())': False, 'contains(they)': False, 'contains(you)': True, 'contains(not)': False, 'contains(have)': False, 'contains(like)': False, 'contains(will)': False, 'contains(all)': False, 'contains(into)': False, 'contains(out)': False, 'contains(she)': False, 'contains(what)': False, 'contains(life)': False, 'contains(has)': False, 'contains(its)': False, 'contains(only)': False, 'contains(more)': False, 'contains(even)': False, 'contains(--)': False, 'contains(:)': False, 'contains(can)': False, 'contains(;)': False, 'contains(home)': True, 'contains(look)': False, \"contains(it's)\": False, 'contains(if)': False, 'contains(where)': False, 'contains(most)': False, 'contains(him)': False, 'contains(search)': False, 'contains(but_NEG)': False, 'contains(love)': False, 'contains(both)': False, 'contains(make)': False, 'contains(begins)': False, 'contains(some)': False, 'contains(two)': False, 'contains(of_NEG)': False, 'contains(made)': False, 'contains(which)': False, 'contains(them)': False}, 'subj'), ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that it has marked each word in each doc, saying which of the top words are in the doc and which are not\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier using the training data\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Print out the evaluation metrics\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "     print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the model\n",
    "We can take a look at the features that the model has homed in on.  These features are the most important ones for distinguishing between subjective and objective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(her) = True              obj : subj   =      5.4 : 1.0\n",
      "           contains(its) = True             subj : obj    =      5.0 : 1.0\n",
      "          contains(more) = True             subj : obj    =      4.3 : 1.0\n",
      "            contains(if) = True             subj : obj    =      3.7 : 1.0\n",
      "            contains(it) = True             subj : obj    =      3.1 : 1.0\n",
      "        contains(begins) = True              obj : subj   =      3.0 : 1.0\n",
      "          contains(both) = True             subj : obj    =      3.0 : 1.0\n",
      "           contains(him) = True              obj : subj   =      3.0 : 1.0\n",
      "           contains(his) = True              obj : subj   =      3.0 : 1.0\n",
      "          contains(life) = True              obj : subj   =      3.0 : 1.0\n",
      "          contains(make) = True             subj : obj    =      3.0 : 1.0\n",
      "            contains(on) = True              obj : subj   =      3.0 : 1.0\n",
      "           contains(she) = True              obj : subj   =      3.0 : 1.0\n",
      "         contains(which) = True              obj : subj   =      3.0 : 1.0\n",
      "            contains(he) = True              obj : subj   =      2.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj\n",
      "subj\n",
      "subj\n",
      "obj\n",
      "obj\n",
      "subj\n",
      "subj\n",
      "obj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "obj\n",
      "subj\n",
      "subj\n",
      "obj\n",
      "obj\n",
      "subj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "subj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "subj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "subj\n"
     ]
    }
   ],
   "source": [
    "# We can check the classification of each test doc\n",
    "for doc in test_set:\n",
    "    print(classifier.classify(doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also classify some \"unseen\" documents, from the subjectivity corpus documents we did not use in the model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some unseen documents\n",
    "subj_docs_unseen = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[n_instances:n_instances+5]]\n",
    "obj_docs_unseen = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[n_instances:n_instances+5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"it's\",\n",
       "  'a',\n",
       "  'frightful',\n",
       "  'vanity',\n",
       "  'film',\n",
       "  'that',\n",
       "  ',',\n",
       "  'no',\n",
       "  'doubt',\n",
       "  ',',\n",
       "  'pays',\n",
       "  'off',\n",
       "  'what',\n",
       "  'debt',\n",
       "  'miramax',\n",
       "  'felt',\n",
       "  'they',\n",
       "  'owed',\n",
       "  'to',\n",
       "  'benigni',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs_unseen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same feature extractor to the unseen documents\n",
    "subj_docs_unseen = sentim_analyzer.apply_features(subj_docs_unseen)\n",
    "obj_docs_unseen = sentim_analyzer.apply_features(obj_docs_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be Subj:\n",
      "subj\n",
      "obj\n",
      "subj\n",
      "subj\n",
      "subj\n",
      "Should be Obj:\n",
      "obj\n",
      "obj\n",
      "obj\n",
      "subj\n",
      "obj\n"
     ]
    }
   ],
   "source": [
    "# Classify them\n",
    "print(\"Should be Subj:\")\n",
    "for doc in subj_docs_unseen:\n",
    "    print(classifier.classify(doc[0]))\n",
    "    \n",
    "print(\"Should be Obj:\")\n",
    "for doc in obj_docs_unseen:\n",
    "    print(classifier.classify(doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Contained example\n",
    "https://data-and-design.readthedocs.io/en/latest/09-Machine-Learning-Intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "train = [(\"Great place to be when you are in Bangalore.\", \"pos\"),\n",
    "  (\"The place was being renovated when I visited so the seating was limited.\", \"neg\"),\n",
    "  (\"Loved the ambience, loved the food\", \"pos\"),\n",
    "  (\"The food is delicious but not over the top.\", \"neg\"),\n",
    "  (\"Service - Little slow, probably because too many people.\", \"neg\"),\n",
    "  (\"The place is not easy to locate\", \"neg\"),\n",
    "  (\"Mushroom fried rice was spicy\", \"pos\"),\n",
    "]\n",
    "dictionary = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n",
    "dictionary = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in train]\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "test_data = \"Manchurian was hot and spicy\"\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "print (classifier.classify(test_data_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
