{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "### Import BeautifulSoup\n",
    "\n",
    "First off, you will need to import the BeautifulSoup library. BS is not part of the Python standard library (i.e. it needs to be installed separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import beautiful soup library\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with BeautifulSoup, you first require some HTML. HTML can either be loaded from a locally stored file, or it can be \\`requested' from a web server over HTTP.\n",
    "To use the second approach, we will utilise another Python library called `requests`, which is able to make and handle HTTP requests and responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests library\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the get method in the requests library to retrieve an HTTP response object. An HTTP request contains header fields which may give the server some additional information about the request. One of the fields is called, `user-agent', and it tells the server what software is making the request on behalf of the user. It may be a good idea to set this header, to try to `fool' the server into believing the request is coming via a browser.\n",
    "\n",
    "The response object has a property, text, which contains the HTML that was sent in the response.\n",
    "\n",
    "In the following example, we can grab the Coursera homepage code. For these labs you will only be able to grab Coursera resources - if you want to explore other exercises you will have to run Jupyter on your own machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a user-agent to be sent with request\n",
    "#headers = {\n",
    "#    \"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\"\n",
    "#}\n",
    "# request a resource from a specific URL. Change this for your chosen website.\n",
    "r  = requests.get(\"https://www.coursera.org/\")#,headers)\n",
    "\n",
    "# put the text that is returned in the response in a variable\n",
    "data = r.text\n",
    "\n",
    "# look...some HTML has been sent in the response!\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw HTML is not very easy to work with, because it is in a semantic markup format. We also have lots of other bits of things in here like CSS! \n",
    "\n",
    "We need to \\`parse' the HTML (i.e. split it into its component parts), which will make working with it much easier. For that we will create an object which is an instance of the BeautifulSoup class. The object will be a special kind of data structure. It will contain the HTML, but in a format we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# parse the raw HTML into a `soup' object\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have parsed the HTML, we can call methods of the BeautifulSoup class to access specific elements in the data.\n",
    "\n",
    "### Extract a single element by tag name\n",
    "For example, the `find` method will return the first available element with a specified tag name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.find(\"h1\")\n",
    "print(h1)\n",
    "# If you can't find the h1 element you might want to right click the page and view source. See if you can identify \n",
    "# an element you can scrape e.g. <p> tags <img> tags or links <a href=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all of a certain element by tag name\n",
    "The `find_all` method will return all the elements of a certain type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the th elements \n",
    "# e.g. for a table we could say\n",
    "# table = soup.find_all(\"table\")\n",
    "text = soup.find_all(\"p\")\n",
    "\n",
    "# Not all web pages will have tables, p tags or classes. Markup is dependent on what the person building the \n",
    "# webpage decided should appear.\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter elements by attribute\n",
    "\n",
    "HTML elements can have attributes. These are key-value pairs defined inside the opening tag. For example, a hyperlink (anchor) tag has an href attribute specifying the URL to link to:\n",
    "\n",
    "        <a href=\"http://www.somewhereoutthere.com\">This is not a real URL!</a>\n",
    "        \n",
    "We can be more specific about which elements to retrieve with find all, by including an attribute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the th elements containing the scope attribute, with the value, `row'\n",
    "# rows = table[0].find_all(\"tr\")\n",
    "# Let's get the element in our text variable at index 10\n",
    "elements = text[10]\n",
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter elements by contents\n",
    "We may also decide which elements to extract based on their text contents. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all th elements containing the string, `Salt'\n",
    "# energy = table[0].find_all(\"td\",string=\"Energy \")\n",
    "# energy\n",
    "# Can you do this for text[10] or one of the other elements in our text variable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the next sibling element\n",
    "We might want to get at the element next to another element. \n",
    "\n",
    "For example, let's suppose I want the value contained in the `td` element proceding the previous `th`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text from the next element after something\n",
    "# as a table you might do energy[0].findNext(\"td\").text\n",
    "# or we could do this\n",
    "elements.findNext(\"\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have a look at the rest of the page\n",
    "Does this look easy to scrape? Are there any elements that look confusing or strange?\n",
    "\n",
    "Select an element on the page and write a line of code that is able to scrape that element. Post your code on the discussion forums and explain how it works. Comment on a post from one of your colleagues and see if you can replicate their web scraping exercise.\n",
    "\n",
    "You will notice that the page has lots of Javascript generating what we describe as 'dynamic events.' Post your thoughts on the discussion forums regarding:\n",
    "<ul><li>What do you think the Javascript is doing?</li>\n",
    "    <li>Why do you think it is used/useful?</li>\n",
    "    <li>What challenges might you face in web scraping dynamic content?</li>\n",
    "    </ul>\n",
    "Reply to a post from one of your colleagues and see if you agree with their findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "For a more detailed introduction to web scraping, you may find this [Webscraping article](https://blog.hartleybrody.com/web-scraping/) by Hartley Brody interesting.\n",
    "\n",
    "The BeautifulSoup documentation can be found here: [BS Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcc6fa51be0ad111bbc12f6c1c3e54930c34b0f1ae03fb7b761eacf59634953"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
